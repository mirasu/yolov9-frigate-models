# Configuraci贸n de YOLOv9 para Frigate

Has compilado con 茅xito tres variantes del modelo **YOLOv9** a formato ONNX.

## Ubicaci贸n de los archivos
- **YOLOv9-E (Extended/Power):** `/home/mirasu/yolov9-p.onnx` (265 MB) - *M谩xima precisi贸n*
- **YOLOv9-S (Small):** `/home/mirasu/yolov9-s.onnx` (39 MB) - *Equilibrio*
- **YOLOv9-T (Tiny):** `/home/mirasu/yolov9-t.onnx` (9 MB) - *M谩xima velocidad*

## Integraci贸n en Frigate (`config.yml`)

Elige el modelo seg煤n tu hardware. El modelo **Tiny** es excelente para CPUs poco potentes, mientras que el **Extended (p)** es ideal si tienes un procesador moderno o iGPU.

### Ejemplo de configuraci贸n
```yaml
detectors:
  ov:
    type: openvino
    device: AUTO

model:
  path: /config/model_cache/yolov9-s.onnx # Cambia a -p o -t seg煤n prefieras
  input_tensor: nchw
  input_pixel_format: rgb
  width: 640
  height: 640
  labelmap_path: /config/model_cache/coco_labels.txt
```

### Diferencias de rendimiento
- **Tiny (t):** Muy bajo consumo de CPU, ideal para muchas c谩maras.
- **Small (s):** Buen punto medio para detecci贸n de personas/objetos peque帽os.
- **Extended (p):** Pesado, pero con la mejor detecci贸n disponible en YOLOv9.


### Notas Importantes
1. **Labels:** YOLOv9 usa las 80 clases de COCO. Si no tienes el archivo de etiquetas, puedes crearlo con la lista est谩ndar de COCO (person, bicycle, car, motorcyle, etc.).
2. **Dimensiones:** El modelo se export贸 con una resoluci贸n de **640x640**. Aseg煤rate de que coincida en la configuraci贸n.
3. **Rutas:** Recuerda mapear el volumen en Docker para que Frigate vea el archivo `.onnx` (ejemplo: `-v /home/mirasu/yolov9-p.onnx:/config/model_cache/yolov9-p.onnx:ro`).

### Conversi贸n a TensorRT (.engine)
Si decides usar una GPU Nvidia, puedes convertir el ONNX a un engine optimizado usando el script proporcionado:
```bash
python3 /workspace/convert_onnx_to_trt.py --onnx /home/mirasu/yolov9-p.onnx --output /home/mirasu/yolov9-p.engine
```
Y luego cambia el detector en Frigate a `tensorrt`.

---

##  Uso con Docker

Si prefieres usar la imagen Docker para distribuir o servir los modelos, sigue estos pasos:

### 1. Construir la imagen
Desde la ra铆z del repositorio:
```bash
docker build -t yolov9-models-frigate:latest .
```

### 2. Ejecutar y verificar
La imagen est谩 basada en Alpine y simplemente contiene los archivos en `/models`.
```bash
docker run --rm yolov9-models-frigate:latest ls -lh /models
```

### 3. Extraer modelos desde Docker (opcional)
Si necesitas sacar un modelo espec铆fico de la imagen construida:
```bash
docker run --rm -v $(pwd):/output yolov9-models-frigate:latest cp /models/yolov9-s-320.onnx /output/
```

---

##  Notas sobre el Hardware
- **CPU / OpenVINO:** Recomendado usar los modelos de **320px** para menor latencia.
- **GPU / TensorRT:** Los modelos de **640px** ofrecen la mejor precisi贸n si tienes potencia suficiente.
